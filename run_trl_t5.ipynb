{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc3a7d5a-6360-4802-9353-c55bcde6d433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.8.1\n",
      "    Uninstalling huggingface-hub-0.8.1:\n",
      "      Successfully uninstalled huggingface-hub-0.8.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.20.1\n",
      "    Uninstalling transformers-4.20.1:\n",
      "      Successfully uninstalled transformers-4.20.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.3.2\n",
      "    Uninstalling datasets-2.3.2:\n",
      "      Successfully uninstalled datasets-2.3.2\n",
      "Successfully installed accelerate-0.16.0 appdirs-1.4.4 datasets-2.9.0 deepspeed-0.8.0 distlib-0.3.6 docker-pycreds-0.4.0 hjson-3.1.0 huggingface-hub-0.12.1 ninja-1.11.1 numpy-1.24.2 pathtools-0.1.2 py-cpuinfo-9.0.0 sentry-sdk-1.15.0 setproctitle-1.3.2 transformers-4.24.0 urllib3-1.26.14 wandb-0.13.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f6b28-b3cd-4335-8a6b-96f379f70a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/justinphan3110/SciFive.git\n",
    "# !cp -r SciFive/biot5x/data .\n",
    "# !rm -r SciFive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e1441a6-9c76-4754-82e8-a75cccafa2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead, set_seed\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dec8892-03d8-471c-ad2d-9dcac1c6edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"justinphan3110/biot5_chemprot\",\n",
    "    # model_name=\"t5-base\",\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=1024,\n",
    "    forward_batch_size=1024,\n",
    "    eval_batch_size=512,\n",
    "    input_length = 256,\n",
    "    target_length = 5,\n",
    "    metric = 'PRF1',\n",
    "    ppo_epochs=1,\n",
    "    init_kl_coef=0.0,\n",
    "    log_with=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    accelerator_kwargs={\"logging_dir\": \"log\"}\n",
    ")\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": config.forward_batch_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35157d4f-709c-4644-acd1-91e113b3bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed before initializing value head for deterministic eval\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Now let's build the model, the reference model, and the tokenizer.\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)\n",
    "ref_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afa45ac1-04aa-4408-9e3a-32b23078835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, input_length=config.input_length, target_length=config.target_length):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"inputs\"], max_length=input_length, truncation=True, padding=True\n",
    "    )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "      labels = tokenizer(\n",
    "          examples[\"labels\"], max_length=target_length, truncation=True, padding=True\n",
    "      )\n",
    "\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    model_inputs['input_ids'] = model_inputs['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a071eda1-9382-4c6a-a01b-11165fed1f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f557b2d90084d6b85938e90ba53e063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc436eb28a44defb09a42ba56560e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da3ca53ed224cb6b44f80e17d5848a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_column = \"inputs\"\n",
    "target_column = \"labels\"\n",
    "raw_datasets = {}\n",
    "task = \"chemprot\"\n",
    "id2label = {}\n",
    "for line in open(f'data/{task}/label2id.tsv'):\n",
    "    line = line.strip().split('\\t')\n",
    "    id_ = line[1]\n",
    "    label = line[0]\n",
    "    id2label[id_]=label\n",
    "\n",
    "for file_ in ['train','test','dev']:\n",
    "    with open(f\"data/{task}/{file_}_blurb_text.tsv\", \"w\") as out_file:\n",
    "        with open(f\"data/{task}/{file_}_blurb.tsv\", \"r\") as file:\n",
    "            for line in file:\n",
    "                line = line.strip().split('\\t')\n",
    "                input_ = line[0]\n",
    "                target = id2label[line[1]]\n",
    "                out_file.write(f\"{input_}\\t{target}\\n\")\n",
    "                \n",
    "\n",
    "\n",
    "for file_ in ['train', 'dev', 'test']:\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    with open(f'data/{task}/{file_}_blurb_text.tsv', 'r', encoding='utf-8') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            line = line.strip().split('\\t')\n",
    "            inputs.append(f'{line[0].strip()}')\n",
    "            targets.append(f'{line[1].strip()}')\n",
    "    \n",
    "    \n",
    "    dataset = Dataset.from_dict({input_column: inputs, target_column: targets})\n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=['inputs'], num_proc=1)\n",
    "    tokenized_datasets.set_format(type=\"torch\")\n",
    "    raw_datasets[file_] = tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "999dc7dc-1545-4eb4-b711-2774a69ebf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/accelerate/accelerator.py:231: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.\n",
      "  warnings.warn(\n",
      "WARNING:root:Forward batch size > 1 is not well supported yet for encoder-decoder models and when using `tokenizer.padding_side='left'`. This can lead to unexpected behaviour. therefore, we recommend using forward_batch_size=1.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at justinphan3110/biolinkbert_chemprot and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "## IMPORTANT: Need to be a multiple of batch size\n",
    "train_datasets =  Dataset.from_dict(raw_datasets['train'][:512*20])\n",
    "\n",
    "train_datasets.set_format(type=\"torch\")\n",
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=train_datasets, data_collator=data_collator)\n",
    "\n",
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "    \n",
    "    \n",
    "pipe = pipeline(\"text-classification\")\n",
    "classification_pipe = pipeline(\"text-classification\", \"justinphan3110/biolinkbert_chemprot\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6008ce89-a6b2-46b8-98c8-e4820e61bc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 31/31 [00:54<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 78.2702, 'recall': 74.1399, 'F1': 76.1491}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ppo_trainer.evaluate(raw_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3460ce-9fbc-424f-865c-4f991425c729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/transformers/pipelines/text_classification.py:89: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "/notebooks/ensemble_trl/trl/trainer/ppo_trainer.py:876: UserWarning: The game logs will not be logged because the batch does not contain the keys 'query' and 'response'. \n",
      "  warnings.warn(\n",
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 1/31 [00:01<00:43,  1.46s/it]\u001b[A\n",
      "  6%|▋         | 2/31 [00:02<00:42,  1.46s/it]\u001b[A\n",
      " 10%|▉         | 3/31 [00:04<00:40,  1.44s/it]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:06<00:45,  1.68s/it]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:08<00:47,  1.82s/it]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:10<00:47,  1.90s/it]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:11<00:39,  1.63s/it]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:13<00:40,  1.77s/it]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:15<00:40,  1.86s/it]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:17<00:40,  1.93s/it]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:19<00:39,  1.98s/it]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:21<00:38,  2.01s/it]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:23<00:34,  1.93s/it]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:25<00:31,  1.88s/it]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:26<00:26,  1.69s/it]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:28<00:25,  1.73s/it]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:30<00:24,  1.76s/it]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:32<00:23,  1.78s/it]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:33<00:20,  1.68s/it]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:35<00:19,  1.80s/it]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:37<00:18,  1.89s/it]\u001b[A\n",
      " 71%|███████   | 22/31 [00:39<00:17,  1.95s/it]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:41<00:14,  1.80s/it]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:42<00:11,  1.70s/it]\u001b[A\n",
      " 81%|████████  | 25/31 [00:44<00:09,  1.62s/it]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:45<00:08,  1.62s/it]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:47<00:06,  1.62s/it]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:49<00:05,  1.76s/it]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:51<00:03,  1.86s/it]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:53<00:01,  1.92s/it]\u001b[A\n",
      "100%|██████████| 31/31 [00:54<00:00,  1.77s/it]\u001b[A\n",
      " 10%|█         | 1/10 [02:24<21:44, 144.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 78.2622, 'recall': 74.3149, 'F1': 76.2375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 1/31 [00:01<00:43,  1.46s/it]\u001b[A\n",
      "  6%|▋         | 2/31 [00:02<00:42,  1.47s/it]\u001b[A\n",
      " 10%|▉         | 3/31 [00:04<00:40,  1.44s/it]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:06<00:45,  1.69s/it]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:08<00:47,  1.83s/it]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:10<00:49,  1.98s/it]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:11<00:40,  1.68s/it]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:13<00:41,  1.80s/it]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:15<00:41,  1.89s/it]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:18<00:40,  1.95s/it]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:20<00:39,  2.00s/it]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:22<00:38,  2.02s/it]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:23<00:34,  1.94s/it]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:25<00:32,  1.88s/it]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:26<00:27,  1.69s/it]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:28<00:25,  1.73s/it]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:30<00:24,  1.76s/it]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:32<00:23,  1.78s/it]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:33<00:20,  1.68s/it]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:35<00:19,  1.80s/it]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:38<00:18,  1.88s/it]\u001b[A\n",
      " 71%|███████   | 22/31 [00:40<00:17,  1.94s/it]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:41<00:14,  1.80s/it]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:43<00:11,  1.70s/it]\u001b[A\n",
      " 81%|████████  | 25/31 [00:44<00:09,  1.62s/it]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:46<00:08,  1.62s/it]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:47<00:06,  1.61s/it]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:49<00:05,  1.75s/it]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:51<00:03,  1.85s/it]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:54<00:01,  1.92s/it]\u001b[A\n",
      "100%|██████████| 31/31 [00:55<00:00,  1.77s/it]\u001b[A\n",
      " 20%|██        | 2/10 [04:48<19:14, 144.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 77.9945, 'recall': 74.6064, 'F1': 76.2629}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 1/31 [00:01<00:43,  1.46s/it]\u001b[A\n",
      "  6%|▋         | 2/31 [00:02<00:42,  1.45s/it]\u001b[A\n",
      " 10%|▉         | 3/31 [00:04<00:40,  1.44s/it]\u001b[A\n",
      " 13%|█▎        | 4/31 [00:06<00:45,  1.69s/it]\u001b[A\n",
      " 16%|█▌        | 5/31 [00:08<00:47,  1.83s/it]\u001b[A\n",
      " 19%|█▉        | 6/31 [00:10<00:47,  1.92s/it]\u001b[A\n",
      " 23%|██▎       | 7/31 [00:11<00:39,  1.65s/it]\u001b[A\n",
      " 26%|██▌       | 8/31 [00:13<00:41,  1.79s/it]\u001b[A\n",
      " 29%|██▉       | 9/31 [00:15<00:41,  1.88s/it]\u001b[A\n",
      " 32%|███▏      | 10/31 [00:17<00:41,  1.95s/it]\u001b[A\n",
      " 35%|███▌      | 11/31 [00:20<00:40,  2.00s/it]\u001b[A\n",
      " 39%|███▊      | 12/31 [00:22<00:38,  2.03s/it]\u001b[A\n",
      " 42%|████▏     | 13/31 [00:23<00:35,  1.95s/it]\u001b[A\n",
      " 45%|████▌     | 14/31 [00:25<00:32,  1.90s/it]\u001b[A\n",
      " 48%|████▊     | 15/31 [00:26<00:27,  1.71s/it]\u001b[A\n",
      " 52%|█████▏    | 16/31 [00:28<00:26,  1.75s/it]\u001b[A\n",
      " 55%|█████▍    | 17/31 [00:30<00:24,  1.78s/it]\u001b[A\n",
      " 58%|█████▊    | 18/31 [00:32<00:23,  1.80s/it]\u001b[A\n",
      " 61%|██████▏   | 19/31 [00:33<00:20,  1.70s/it]\u001b[A\n",
      " 65%|██████▍   | 20/31 [00:36<00:20,  1.82s/it]\u001b[A\n",
      " 68%|██████▊   | 21/31 [00:38<00:19,  1.91s/it]\u001b[A\n",
      " 71%|███████   | 22/31 [00:40<00:17,  1.97s/it]\u001b[A\n",
      " 74%|███████▍  | 23/31 [00:41<00:14,  1.82s/it]\u001b[A\n",
      " 77%|███████▋  | 24/31 [00:43<00:12,  1.73s/it]\u001b[A\n",
      " 81%|████████  | 25/31 [00:44<00:09,  1.64s/it]\u001b[A\n",
      " 84%|████████▍ | 26/31 [00:46<00:08,  1.64s/it]\u001b[A\n",
      " 87%|████████▋ | 27/31 [00:48<00:06,  1.64s/it]\u001b[A\n",
      " 90%|█████████ | 28/31 [00:50<00:05,  1.78s/it]\u001b[A\n",
      " 94%|█████████▎| 29/31 [00:52<00:03,  1.88s/it]\u001b[A\n",
      " 97%|█████████▋| 30/31 [00:54<00:01,  1.95s/it]\u001b[A\n",
      "100%|██████████| 31/31 [00:55<00:00,  1.79s/it]\u001b[A\n",
      " 30%|███       | 3/10 [07:12<16:49, 144.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 77.6834, 'recall': 74.6939, 'F1': 76.1593}\n"
     ]
    }
   ],
   "source": [
    "# We then define the arguments to pass to the `generate` function. These arguments\n",
    "# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
    "# the `generate` function of the trained model.\n",
    "output_min_length = 2\n",
    "output_max_length = config.target_length\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "generation_kwargs = { \"max_length\": config.target_length}\n",
    "dataloader = torch.utils.data.DataLoader(train_datasets, collate_fn=data_collator, batch_size=config.forward_batch_size)\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    out_dir = f\"out/test_trl_biot5_{task}/checkpoint_{epoch}\"\n",
    "    for batch in tqdm(ppo_trainer.dataloader):\n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "        label_tensors = batch[\"labels\"]\n",
    "        \n",
    "        outputs = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "        response_tensors = list(outputs)\n",
    "        texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        labels = tokenizer.batch_decode(label_tensors, skip_special_tokens=True)\n",
    "\n",
    "        sent_kwargs['function_to_apply'] = 'sigmoid'\n",
    "        sent_kwargs['return_all_scores'] = True\n",
    "        pipe_outputs = classification_pipe(texts, **sent_kwargs)\n",
    "        \n",
    "        rewards = []\n",
    "        for t,output, label in zip(texts, pipe_outputs, labels):\n",
    "            if label == t:\n",
    "                if label == '0':\n",
    "                    reward = 0.0\n",
    "                else:\n",
    "                    reward = 1.0\n",
    "            else:\n",
    "                if label == '0':\n",
    "                    reward = 0.0\n",
    "                else: \n",
    "                    reward = 0.0\n",
    "            rewards.append(torch.tensor(reward).to(device))\n",
    "        \n",
    "        assert len(rewards) == len(labels) == len(texts)\n",
    "        \n",
    "        # print(rewards)\n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step(list(query_tensors), response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "        ppo_trainer.evaluate(raw_datasets['test'])\n",
    "    # print(f\"saving pretrained epoch {epoch} to {out_dir}\")\n",
    "    # ppo_trainer._save_pretrained(out_dir)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290075ca-2a8a-4f9d-95e9-211e49b1163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
