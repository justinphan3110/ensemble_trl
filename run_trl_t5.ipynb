{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db960595-d0af-410f-9f8c-b434803d0ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed accelerate-0.16.0 appdirs-1.4.4 docker-pycreds-0.4.0 pathtools-0.1.2 sentry-sdk-1.15.0 setproctitle-1.3.2 urllib3-1.26.14 wandb-0.13.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/justinphan3110/SciFive.git\n",
    "!cp -r SciFive/biot5x/data .\n",
    "!rm -rm SciFive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7181cc-9263-4b54-a375-784f8d0758ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead, set_seed\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183e44c7-df4b-436d-bb82-d3c2d524f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"out/chemprot_hf/biot5_pytorch_text\",\n",
    "    # model_name=\"t5-base\",\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=1024,\n",
    "    forward_batch_size=1024,\n",
    "    input_length = 256,\n",
    "    target_length = 5,\n",
    "    metric = 'PRF1',\n",
    "    ppo_epochs=1,\n",
    "    init_kl_coef=0.0,\n",
    "    log_with=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    accelerator_kwargs={\"logging_dir\": \"log\"}\n",
    ")\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": config.forward_batch_size}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc69c3-b3a5-4873-adda-aaa49c2472e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set seed before initializing value head for deterministic eval\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Now let's build the model, the reference model, and the tokenizer.\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)\n",
    "ref_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e00817-e34e-4b78-b435-9ce9ff724946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, input_length=config.input_length, target_length=config.target_length):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"inputs\"], max_length=input_length, truncation=True, padding=True\n",
    "    )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "      labels = tokenizer(\n",
    "          examples[\"labels\"], max_length=target_length, truncation=True, padding=True\n",
    "      )\n",
    "\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    model_inputs['input_ids'] = model_inputs['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61778ebe-a380-4c5c-8ba1-70fec7eb041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_column = \"inputs\"\n",
    "target_column = \"labels\"\n",
    "raw_datasets = {}\n",
    "for file_ in ['train', 'dev', 'test']:\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    with open(f'data/chemprot/{file_}_blurb_text.tsv', 'r', encoding='utf-8') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            line = line.strip().split('\\t')\n",
    "            inputs.append(f'{line[0].strip()}')\n",
    "            targets.append(f'{line[1].strip()}')\n",
    "    \n",
    "    \n",
    "    dataset = Dataset.from_dict({input_column: inputs, target_column: targets})\n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=['inputs'], num_proc=1)\n",
    "    tokenized_datasets.set_format(type=\"torch\")\n",
    "    raw_datasets[file_] = tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01698355-008e-44d1-a5f5-57bc470b26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n",
    "\n",
    "train_datasets =  Dataset.from_dict(raw_datasets['test'][:512*20])\n",
    "train_datasets.set_format(type=\"torch\")\n",
    "\n",
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=train_datasets, data_collator=data_collator)\n",
    "\n",
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "    print(\"device\", device)\n",
    "    \n",
    "    \n",
    "pipe = pipeline(\"text-classification\")\n",
    "classification_pipe = pipeline(\"text-classification\", \"out/chemprot_hf/BioLinkBERT-base\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c139b-5154-4fbc-9587-ced52f933bdb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We then define the arguments to pass to the `generate` function. These arguments\n",
    "# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
    "# the `generate` function of the trained model.\n",
    "output_min_length = 2\n",
    "output_max_length = 5\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "generation_kwargs = { \"max_length\": 5}\n",
    "dataloader = torch.utils.data.DataLoader(train_datasets, collate_fn=data_collator, batch_size=config.forward_batch_size)\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    out_dir = f\"out/test_trl_biot5_chemprot/checkpoint_{epoch}\"\n",
    "    for batch in tqdm(ppo_trainer.dataloader):\n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "        label_tensors = batch[\"labels\"]\n",
    "\n",
    "        outputs = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "        response_tensors = list(outputs)\n",
    "        texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        labels = tokenizer.batch_decode(label_tensors, skip_special_tokens=True)\n",
    "\n",
    "        sent_kwargs['function_to_apply'] = 'sigmoid'\n",
    "        sent_kwargs['return_all_scores'] = True\n",
    "        pipe_outputs = classification_pipe(texts, **sent_kwargs)\n",
    "        \n",
    "        rewards = []\n",
    "        for t,output, label in zip(texts, pipe_outputs, labels):\n",
    "            if label == t:\n",
    "                if label == '0':\n",
    "                    reward = 1.0\n",
    "                else:\n",
    "                    reward = 1.0\n",
    "            else:\n",
    "                if label == '0':\n",
    "                    reward = 0\n",
    "                else: \n",
    "                    reward = 0\n",
    "            rewards.append(torch.tensor(reward).to(device))\n",
    "        \n",
    "        assert len(rewards) == len(labels) == len(texts)\n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step(list(query_tensors), response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "    print(f\"saving pretrained epoch {epoch} to {out_dir}\")\n",
    "    ppo_trainer._save_pretrained(out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "eedb55a3f3d5a08c90a45b02edd9d5201f64a9996f64fdac14a22b56503f46e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
