{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a7d5a-6360-4802-9353-c55bcde6d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f6b28-b3cd-4335-8a6b-96f379f70a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/justinphan3110/SciFive.git\n",
    "!cp -r SciFive/biot5x/data .\n",
    "!rm -r SciFive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1441a6-9c76-4754-82e8-a75cccafa2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead, set_seed\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec8892-03d8-471c-ad2d-9dcac1c6edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"justinphan3110/biot5_chemprot\",\n",
    "    # model_name=\"t5-base\",\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=256,\n",
    "    forward_batch_size=256,\n",
    "    eval_batch_size=32,\n",
    "    input_length = 256,\n",
    "    target_length = 5,\n",
    "    metric = 'PRF1',\n",
    "    ppo_epochs=1,\n",
    "    init_kl_coef=0.0,\n",
    "    log_with=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    accelerator_kwargs={\"logging_dir\": \"log\"}\n",
    ")\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": config.forward_batch_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35157d4f-709c-4644-acd1-91e113b3bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed before initializing value head for deterministic eval\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Now let's build the model, the reference model, and the tokenizer.\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)\n",
    "ref_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa45ac1-04aa-4408-9e3a-32b23078835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, input_length=config.input_length, target_length=config.target_length):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"inputs\"], max_length=input_length, truncation=True, padding=True\n",
    "    )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "      labels = tokenizer(\n",
    "          examples[\"labels\"], max_length=target_length, truncation=True, padding=True\n",
    "      )\n",
    "\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    model_inputs['input_ids'] = model_inputs['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a071eda1-9382-4c6a-a01b-11165fed1f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_column = \"inputs\"\n",
    "target_column = \"labels\"\n",
    "raw_datasets = {}\n",
    "task = \"chemprot\"\n",
    "id2label = {}\n",
    "for line in open(f'data/{task}/label2id.tsv'):\n",
    "    line = line.strip().split('\\t')\n",
    "    id_ = line[1]\n",
    "    label = line[0]\n",
    "    id2label[id_]=label\n",
    "\n",
    "for file_ in ['train','test','dev']:\n",
    "    with open(f\"data/{task}/{file_}_blurb_text.tsv\", \"w\") as out_file:\n",
    "        with open(f\"data/{task}/{file_}_blurb.tsv\", \"r\") as file:\n",
    "            for line in file:\n",
    "                line = line.strip().split('\\t')\n",
    "                input_ = line[0]\n",
    "                target = id2label[line[1]]\n",
    "                out_file.write(f\"{input_}\\t{target}\\n\")\n",
    "                \n",
    "\n",
    "\n",
    "for file_ in ['train', 'dev', 'test']:\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    with open(f'data/{task}/{file_}_blurb_text.tsv', 'r', encoding='utf-8') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            line = line.strip().split('\\t')\n",
    "            inputs.append(f'{line[0].strip()}')\n",
    "            targets.append(f'{line[1].strip()}')\n",
    "    \n",
    "    \n",
    "    dataset = Dataset.from_dict({input_column: inputs, target_column: targets})\n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=['inputs'], num_proc=1)\n",
    "    tokenized_datasets.set_format(type=\"torch\")\n",
    "    raw_datasets[file_] = tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999dc7dc-1545-4eb4-b711-2774a69ebf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "## IMPORTANT: Need to be a multiple of batch size\n",
    "train_datasets =  Dataset.from_dict(raw_datasets['train'][:512*20])\n",
    "\n",
    "train_datasets.set_format(type=\"torch\")\n",
    "\n",
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=train_datasets, data_collator=data_collator)\n",
    "\n",
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "    print(\"device\", device)\n",
    "    \n",
    "    \n",
    "pipe = pipeline(\"text-classification\")\n",
    "classification_pipe = pipeline(\"text-classification\", \"justinphan3110/biolinkbert_chemprot\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008ce89-a6b2-46b8-98c8-e4820e61bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer.evaluate(raw_datasets['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3460ce-9fbc-424f-865c-4f991425c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then define the arguments to pass to the `generate` function. These arguments\n",
    "# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
    "# the `generate` function of the trained model.\n",
    "output_min_length = 2\n",
    "output_max_length = config.target_length\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "generation_kwargs = { \"max_length\": config.target_length}\n",
    "dataloader = torch.utils.data.DataLoader(train_datasets, collate_fn=data_collator, batch_size=config.forward_batch_size)\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    out_dir = f\"out/test_trl_biot5_{task}/checkpoint_{epoch}\"\n",
    "    for batch in tqdm(ppo_trainer.dataloader):\n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "        label_tensors = batch[\"labels\"]\n",
    "\n",
    "        outputs = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "        response_tensors = list(outputs)\n",
    "        texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        labels = tokenizer.batch_decode(label_tensors, skip_special_tokens=True)\n",
    "\n",
    "        sent_kwargs['function_to_apply'] = 'sigmoid'\n",
    "        sent_kwargs['return_all_scores'] = True\n",
    "        pipe_outputs = classification_pipe(texts, **sent_kwargs)\n",
    "        \n",
    "        rewards = []\n",
    "        for t,output, label in zip(texts, pipe_outputs, labels):\n",
    "            if label == t:\n",
    "                if label == '0':\n",
    "                    reward = 1.0\n",
    "                else:\n",
    "                    reward = 1.0\n",
    "            else:\n",
    "                if label == '0':\n",
    "                    reward = 0\n",
    "                else: \n",
    "                    reward = 0\n",
    "            rewards.append(torch.tensor(reward).to(device))\n",
    "        \n",
    "        assert len(rewards) == len(labels) == len(texts)\n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step(list(query_tensors), response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "    print(f\"saving pretrained epoch {epoch} to {out_dir}\")\n",
    "    ppo_trainer._save_pretrained(out_dir)\n",
    "    ppo_trainer.evaluate(raw_dataset['dev'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
